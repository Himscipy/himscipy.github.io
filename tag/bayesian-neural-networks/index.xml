<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian Neural Networks | Himanshu Sharma</title>
    <link>https://himscipy.github.io/tag/bayesian-neural-networks/</link>
      <atom:link href="https://himscipy.github.io/tag/bayesian-neural-networks/index.xml" rel="self" type="application/rss+xml" />
    <description>Bayesian Neural Networks</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â©Himanshu Sharma 2020</copyright><lastBuildDate>Sat, 25 May 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://himscipy.github.io/images/icon_hubbf93ac11c8082ceecfc59197d660a4d_16666_512x512_fill_lanczos_center_2.png</url>
      <title>Bayesian Neural Networks</title>
      <link>https://himscipy.github.io/tag/bayesian-neural-networks/</link>
    </image>
    
    <item>
      <title>Bayesian Neural Networks Distributed Training Performance Analysis at Scale</title>
      <link>https://himscipy.github.io/project/my-project-bnn/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      <guid>https://himscipy.github.io/project/my-project-bnn/</guid>
      <description>&lt;p&gt;&lt;strong&gt;ABSTRACT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bayesian neural Networks (BNNs) are a promising method of obtaining statistical uncertainties for
neural network predictions but with a higher computational overhead which can limit their practical
usage. This work explores the use of high performance computing with distributed training to address
the challenges of training BNNs at scale. We present a performance and scalability comparison of
training the VGG-16 and Resnet-18 models on a Cray-XC40 cluster. We demonstrate that network
pruning can speed up inference without accuracy loss and provide an open source software package,
BPrune to automate this pruning. For certain models we find that pruning up to 80% of the network
results in only a 7.0% loss in accuracy. With the development of new hardware accelerators for Deep
Learning, BNNs are of considerable interest for benchmarking performance. This analysis of training
a BNN at scale outlines the limitations and benefits compared to a conventional neural network.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
